{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import generic_io as gi\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_selection import SelectPercentile, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'precision': make_scorer(precision_score, average='weighted'),\n",
    "           'recall': make_scorer(recall_score, average='weighted')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gi.load_from_file(file_path='veri/models/w2v_model_earthquake_76516.bin', file_format='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_tweet(text):\n",
    "    text = re.sub(\"https?[^\\s]*\", \" http://someurl \", text)\n",
    "    text = re.sub(\"(?<!\\w)@\\w{1,15}(?!\\w)\", \" @someuser \", text)\n",
    "#     text = re.sub(\"\\s\\d*(\\.|\\:)?\\d*\\s\", \" digit \", text)\n",
    "    text = re.sub(\"(\\d+(/|-)\\d+(/|-)\\d+)\", \" date \", text)\n",
    "    text = re.sub(\"\\s\\d?\\d(:|,|.)\\d\\d\\s?(A|a|P|p)(M|m)\", \" clock \", text)\n",
    "#     text = emoji_pattern.sub('[\\U0001f600-\\U0001f650]', \"\", text)\n",
    "    text = re.sub('#.*(\\s|\\n)', \"\", text)\n",
    "    tokenized_tweet = tknzr.tokenize(text.lower())\n",
    "    return tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tweets = []\n",
    "    for level in range(1,3):\n",
    "        for step in range(1,5):\n",
    "            T1 = gi.load_from_file(file_path='veri/training/tweets/SMERP-T' + str(step) + '-level' + str(level) + '-tweets.jsonl', file_format='jsonl')\n",
    "            for t in T1:\n",
    "                tokenized_tweet = modify_tweet(t['text'])\n",
    "                row = {'text': tokenized_tweet, 'label': step}\n",
    "                if row not in tweets:\n",
    "                    tweets.append(row)    \n",
    "    return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[toronto's, italian, community, already, mobil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[@someuser, friends, ,, sound, off, !, everyon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  [toronto's, italian, community, already, mobil...\n",
       "1      1  [@someuser, friends, ,, sound, off, !, everyon..."
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, lowercase=True)),\n",
    "    ('clf', LinearSVC(multi_class='ovr'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(classifier, tdf['text'], tdf['label'], scoring=scoring, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.829619851087\n",
      "precision 0.814832694457\n"
     ]
    }
   ],
   "source": [
    "for x in scores:\n",
    "    if 'test' in x:\n",
    "        print(x[5:], sum(scores[x])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some numbers\n",
    "\n",
    "len(model.wv.vocab): 43867\n",
    "\n",
    "len(long_words): 43010\n",
    "\n",
    "count_eng: 6903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find word couples by contextual similarity and edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = []\n",
    "word_couples = []\n",
    "for word in model.wv.vocab:\n",
    "    sim_frst = None\n",
    "    sim_scnd = None\n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            \n",
    "            long_words.append(word)\n",
    "            \n",
    "            sim_list = model.wv.most_similar(word)\n",
    "            \n",
    "            len_first = len(sim_list[0][0])\n",
    "            ed_first = editdistance.eval(word, sim_list[0][0])\n",
    "            if sim_list[0][1] > 0.80:\n",
    "                if len_first >= 6 and ed_first < 3:\n",
    "                    sim_frst = sim_list[0]\n",
    "                elif 6 > len_first > 1 and ed_first < 2:\n",
    "                    sim_frst = sim_list[0]\n",
    "                \n",
    "            len_scnd = len(sim_list[1][0])\n",
    "            ed_scnd = editdistance.eval(word, sim_list[1][0])\n",
    "            if sim_list[1][1] > 0.80:\n",
    "                if len_scnd >= 6 and ed_scnd < 3:\n",
    "                    sim_scnd = sim_list[1]\n",
    "                elif 6 > len_scnd > 1 and ed_scnd < 2:\n",
    "                    sim_scnd = sim_list[1]\n",
    "                \n",
    "            if sim_frst or sim_scnd:\n",
    "                \n",
    "                if sim_frst and sim_scnd:\n",
    "                    if ed_first > ed_scnd:\n",
    "                        most_sim = sim_scnd\n",
    "                    else:\n",
    "                        most_sim = sim_frst\n",
    "                elif sim_frst and not sim_scnd:\n",
    "                    most_sim = sim_frst\n",
    "                elif sim_scnd and not sim_frst:\n",
    "                    most_sim = sim_scnd\n",
    "                    \n",
    "#                 print(word, most_sim[0], most_sim[1])\n",
    "                couple = {'word': word, 'most_sim': most_sim[0], 'similarity': most_sim[1]}\n",
    "                if couple not in word_couples:\n",
    "                        word_couples.append(couple)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some examples\n",
    "\n",
    "{'most_sim': 'fuck', 'similarity': 0.896193265914917, 'word': 'fucking'},\n",
    " \n",
    " {'most_sim': 'women', 'similarity': 0.9045039415359497, 'word': 'men'},\n",
    " \n",
    " {'most_sim': 'earthquake-hit', 'similarity': 0.884941041469574,'word': 'quake-hit'},\n",
    " \n",
    " {'most_sim': 'reading', 'similarity': 0.9149921536445618, 'word': 'hearing'},\n",
    " \n",
    " {'most_sim': 'horrible', 'similarity': 0.9077590703964233, 'word': 'terrible'},\n",
    " \n",
    " {'most_sim': 'levelling', 'similarity': 0.9155386686325073, 'word': 'leveling'},\n",
    " \n",
    " {'most_sim': 'didnt', 'similarity': 0.8618503212928772, 'word': \"didn't\"},\n",
    " \n",
    " {'most_sim': 'worrying', 'similarity': 0.8041735887527466, 'word': 'worring'},\n",
    " \n",
    " {'most_sim': 'sympathises', 'similarity': 0.9335030317306519, 'word': 'sympathizes'},\n",
    " \n",
    " {'most_sim': 'telling', 'similarity': 0.8421267867088318, 'word': 'calling'},\n",
    " ('#n3rdlife', '#nerdlife'),\n",
    " \n",
    " ('gurdwara', 'gurudwara'),\n",
    " ('gurdwaras', \"gurudwara's\"),\n",
    " ('gurdwaras', 'gurudwaras'),\n",
    " \n",
    " ('northeast', 'north-east'),\n",
    " \n",
    " ('richter', 'ritcher'),\n",
    "\n",
    " ('#emergency', '#emergenza'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the couples to a file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_name = 'first_second_non-en_080_' + str(len(word_couples)) + '.json\n",
    "gi.save_to_file(data_=word_couples, file_path='veri/normalization_lists/' + file_name, file_format='json')\n",
    "print('veri/normalization_lists/' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load couples from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_couples = gi.load_from_file(file_path='veri/normalization_lists/first_second_non-en_080_546.json')\n",
    "len(word_couples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the list of words to be replaced by each other (and convert the list into dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_list = []\n",
    "for w in word_couples:\n",
    "    if not re.match(r'.*\\d+', w['most_sim']):\n",
    "        if len(w['word']) < len(w['most_sim']):\n",
    "            couple = (w['word'], w['most_sim'])\n",
    "        else:\n",
    "            couple = (w['most_sim'], w['word'])\n",
    "        if couple not in norm_list and tuple(reversed(couple)) not in norm_list:\n",
    "            norm_list.append(couple)\n",
    "norm_list.sort()\n",
    "norm_df = pd.DataFrame(norm_list, columns=[\"word\", \"most_sim\"])\n",
    "len(norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = load_data()\n",
    "tdf_list = tdf.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization by Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_words = {}\n",
    "for tweet_dict in tdf_list:\n",
    "    for i, word in enumerate(tweet_dict['text']):\n",
    "        if word in list(norm_df['word']):\n",
    "            rep_word = norm_df.loc[norm_df['word'] == word]['most_sim'].iloc[0]\n",
    "            couple_str = word + '-' + rep_word\n",
    "            if couple_str not in replaced_words:\n",
    "                replaced_words[couple_str] = 0\n",
    "            replaced_words[couple_str] += 1\n",
    "            tweet_dict['text'][i] = rep_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,\n",
       " 1146,\n",
       " {'#scary-#scared': 1,\n",
       "  '<b>-</b>': 1,\n",
       "  'abc-bbc': 16,\n",
       "  'anguish-#anguish': 1,\n",
       "  'ap-afp': 29,\n",
       "  'brits-britons': 12,\n",
       "  'centre-center': 5,\n",
       "  'collapse-collapses': 6,\n",
       "  'comment-comments': 1,\n",
       "  'continue-continues': 18,\n",
       "  'da-d:': 1,\n",
       "  'damages-damaged': 25,\n",
       "  'donating-donations': 19,\n",
       "  'donation-donations': 13,\n",
       "  'ear-eart': 3,\n",
       "  'earthq-earthqu': 5,\n",
       "  'earthqu-earthqua': 2,\n",
       "  'eath-#death': 3,\n",
       "  'effected-affected': 1,\n",
       "  'effort-efforts': 9,\n",
       "  'epicentre-epicenter': 5,\n",
       "  'ever-never': 1,\n",
       "  'favorite-favourite': 1,\n",
       "  'fund-funds': 28,\n",
       "  \"he's-she's\": 1,\n",
       "  'he-she': 6,\n",
       "  'heads-headed': 1,\n",
       "  'hearing-reading': 3,\n",
       "  'ita-ital': 3,\n",
       "  'launch-launches': 1,\n",
       "  'leveled-levelled': 3,\n",
       "  'line-lines': 2,\n",
       "  'magnitute-#magnitude': 1,\n",
       "  'make-take': 8,\n",
       "  'min-0min': 1,\n",
       "  'monday-sunday': 1,\n",
       "  \"morning-morning's\": 18,\n",
       "  \"mornings-morning's\": 1,\n",
       "  'ne-se': 3,\n",
       "  'northeast-north-east': 4,\n",
       "  'nytimes-latimes': 1,\n",
       "  'offer-offers': 7,\n",
       "  'part-parts': 5,\n",
       "  'password-passwords': 18,\n",
       "  'queen-#queen': 2,\n",
       "  'race-raced': 18,\n",
       "  'realize-realise': 1,\n",
       "  'reduced-reduces': 5,\n",
       "  'rescue-rescuers': 138,\n",
       "  'richter-ritcher': 3,\n",
       "  'romanian-#romanians': 2,\n",
       "  'romanians-#romanians': 14,\n",
       "  'sauce-sauces': 1,\n",
       "  'scene-scenes': 4,\n",
       "  'seems-seemed': 2,\n",
       "  'sincere-sincerest': 1,\n",
       "  'southern-northern': 1,\n",
       "  'str-stru': 2,\n",
       "  'sw-nw': 1,\n",
       "  'tbn-@tbn': 1,\n",
       "  'terrible-horrible': 18,\n",
       "  'toll-tolls': 622,\n",
       "  'tron-#tron': 1,\n",
       "  'union-unions': 2,\n",
       "  'voices-#voices': 1,\n",
       "  'wiped-wipes': 3,\n",
       "  'word-words': 2,\n",
       "  'would-could': 5,\n",
       "  'ya-yea': 1,\n",
       "  'yrs-hrs': 1})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaced_words), sum(replaced_words.values()), replaced_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert normalized data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_normalized = pd.DataFrame(tdf_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_norm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, lowercase=True)),\n",
    "    ('clf', LinearSVC(multi_class='ovr'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 10-fold cross-validation with the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_norm = cross_validate(classifier_norm, tdf_normalized['text'], tdf_normalized['label'], scoring=scoring, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scores of SVM with contextually normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.830396518241\n",
      "precision 0.81446420177\n"
     ]
    }
   ],
   "source": [
    "for x in scores_norm:\n",
    "    if 'test' in x:\n",
    "        print(x[5:], sum(scores_norm[x])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 100 #len(word2vec.values().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, lowercase=True)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_emb = Pipeline([\n",
    "    ('tfidf', TfidfEmbeddingVectorizer(w2v)),\n",
    "    ('clf', LinearSVC(multi_class='ovr'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 10-fold cross-validation with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_emb = cross_validate(classifier_emb, tdf['text'], tdf['label'], scoring=scoring, cv=10, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scores of Text Classification with W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.792158245338\n",
      "precision 0.767850891193\n"
     ]
    }
   ],
   "source": [
    "for x in scores_emb:\n",
    "    if 'test' in x:\n",
    "        print(x[5:], sum(scores_emb[x])/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
